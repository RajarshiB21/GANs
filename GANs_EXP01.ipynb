{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-V7YYQPCEZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,img_dim):\n",
        "    super().__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "        nn.Linear(img_dim, 128),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(128,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.disc(x)"
      ],
      "metadata": {
        "id": "rRI2rhnSCZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self,z_dim, img_dim):\n",
        "    super().__init__()\n",
        "    self.gen = nn.Sequential(\n",
        "        nn.Linear(z_dim,256),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(256, img_dim),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.gen(x)"
      ],
      "metadata": {
        "id": "gDjIWqIQC7wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Hyperparameter\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4\n",
        "z_dim = 64 #128, 256\n",
        "image_dim = 28*28*1\n",
        "batch_size = 32\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "5hgghLZoDi0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialisation\n",
        "disc = Discriminator(image_dim).to(device)\n",
        "gen = Generator(z_dim, image_dim).to(device)\n",
        "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
        "transforms = transforms.Compose(\n",
        "      [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ],
      "metadata": {
        "id": "xUe6AblyD9V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Loading \n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform = transforms , download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
        "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
        "step = 0"
      ],
      "metadata": {
        "id": "VNlfBlkNEftp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Training\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (real, _) in enumerate(loader):\n",
        "    real = real.view(-1, 784).to(device)#Reshape, keep the number of examples intact and flatten everything else\n",
        "    batch_size = real.shape[0]\n",
        "\n",
        "    #Training Discriminator > max log(D(real)) + log(1-D(G(z)))\n",
        "    noise = torch.randn(batch_size, z_dim).to(device)\n",
        "    fake = gen(noise)\n",
        "    disc_real = disc(real).view(-1)#D(real)\n",
        "    lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "    disc_fake = disc(fake).view(-1)#D(G(z))\n",
        "    lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "    lossD = (lossD_real + lossD_fake)/2\n",
        "    disc.zero_grad()\n",
        "    lossD.backward(retain_graph=True)\n",
        "    opt_disc.step()\n",
        "\n",
        "    #Training Generator min log(1-D(G(z))) ----- max log(D(G(z)))\n",
        "    output = disc(fake).view(-1)\n",
        "    lossG = criterion(output, torch.ones_like(output))\n",
        "    gen.zero_grad()\n",
        "    lossG.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "\n",
        "    #For Tensorboard\n",
        "    if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
        "                data = real.reshape(-1, 1, 28, 28)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "\n",
        "                writer_fake.add_image(\n",
        "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
        "                )\n",
        "                writer_real.add_image(\n",
        "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
        "                )\n",
        "                step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjAlunUfFGmw",
        "outputId": "e8867558-f0bd-4287-b39b-9ad6afc4b45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/50] Batch 0/1875                       Loss D: 0.7582, loss G: 0.6445\n",
            "Epoch [1/50] Batch 0/1875                       Loss D: 0.5062, loss G: 0.8890\n",
            "Epoch [2/50] Batch 0/1875                       Loss D: 0.4243, loss G: 1.2975\n",
            "Epoch [3/50] Batch 0/1875                       Loss D: 0.6946, loss G: 0.8387\n",
            "Epoch [4/50] Batch 0/1875                       Loss D: 0.5115, loss G: 1.0136\n",
            "Epoch [5/50] Batch 0/1875                       Loss D: 0.9317, loss G: 0.6590\n",
            "Epoch [6/50] Batch 0/1875                       Loss D: 0.5054, loss G: 1.2146\n",
            "Epoch [7/50] Batch 0/1875                       Loss D: 0.8856, loss G: 0.8944\n",
            "Epoch [8/50] Batch 0/1875                       Loss D: 0.6224, loss G: 1.2709\n",
            "Epoch [9/50] Batch 0/1875                       Loss D: 0.9650, loss G: 0.6419\n",
            "Epoch [10/50] Batch 0/1875                       Loss D: 0.6525, loss G: 0.7992\n",
            "Epoch [11/50] Batch 0/1875                       Loss D: 0.6287, loss G: 1.2207\n",
            "Epoch [12/50] Batch 0/1875                       Loss D: 0.8455, loss G: 0.9942\n",
            "Epoch [13/50] Batch 0/1875                       Loss D: 0.5132, loss G: 1.2388\n",
            "Epoch [14/50] Batch 0/1875                       Loss D: 0.6383, loss G: 1.4582\n",
            "Epoch [15/50] Batch 0/1875                       Loss D: 0.5581, loss G: 1.0473\n",
            "Epoch [16/50] Batch 0/1875                       Loss D: 0.4671, loss G: 1.2306\n",
            "Epoch [17/50] Batch 0/1875                       Loss D: 0.6509, loss G: 1.2321\n",
            "Epoch [18/50] Batch 0/1875                       Loss D: 0.7318, loss G: 0.7012\n",
            "Epoch [19/50] Batch 0/1875                       Loss D: 0.4469, loss G: 1.1283\n",
            "Epoch [20/50] Batch 0/1875                       Loss D: 0.5135, loss G: 1.3346\n",
            "Epoch [21/50] Batch 0/1875                       Loss D: 0.7625, loss G: 0.9814\n",
            "Epoch [22/50] Batch 0/1875                       Loss D: 0.6774, loss G: 1.0827\n",
            "Epoch [23/50] Batch 0/1875                       Loss D: 0.5581, loss G: 1.1085\n",
            "Epoch [24/50] Batch 0/1875                       Loss D: 0.7044, loss G: 1.1237\n",
            "Epoch [25/50] Batch 0/1875                       Loss D: 0.8031, loss G: 0.9767\n",
            "Epoch [26/50] Batch 0/1875                       Loss D: 0.8781, loss G: 0.7518\n",
            "Epoch [27/50] Batch 0/1875                       Loss D: 0.5734, loss G: 1.4020\n",
            "Epoch [28/50] Batch 0/1875                       Loss D: 0.8128, loss G: 0.7231\n",
            "Epoch [29/50] Batch 0/1875                       Loss D: 0.7532, loss G: 0.8376\n",
            "Epoch [30/50] Batch 0/1875                       Loss D: 0.4825, loss G: 1.2474\n",
            "Epoch [31/50] Batch 0/1875                       Loss D: 0.5483, loss G: 1.0267\n",
            "Epoch [32/50] Batch 0/1875                       Loss D: 0.6212, loss G: 0.8689\n",
            "Epoch [33/50] Batch 0/1875                       Loss D: 0.6056, loss G: 0.8890\n",
            "Epoch [34/50] Batch 0/1875                       Loss D: 0.7173, loss G: 0.7972\n",
            "Epoch [35/50] Batch 0/1875                       Loss D: 0.5935, loss G: 1.1478\n",
            "Epoch [36/50] Batch 0/1875                       Loss D: 0.5774, loss G: 0.9425\n",
            "Epoch [37/50] Batch 0/1875                       Loss D: 0.6538, loss G: 0.9097\n",
            "Epoch [38/50] Batch 0/1875                       Loss D: 0.5751, loss G: 1.2647\n",
            "Epoch [39/50] Batch 0/1875                       Loss D: 0.5984, loss G: 1.1342\n",
            "Epoch [40/50] Batch 0/1875                       Loss D: 0.7261, loss G: 0.9763\n",
            "Epoch [41/50] Batch 0/1875                       Loss D: 0.6720, loss G: 0.8575\n",
            "Epoch [42/50] Batch 0/1875                       Loss D: 0.6465, loss G: 0.8998\n",
            "Epoch [43/50] Batch 0/1875                       Loss D: 0.6048, loss G: 0.8731\n",
            "Epoch [44/50] Batch 0/1875                       Loss D: 0.7072, loss G: 0.8270\n",
            "Epoch [45/50] Batch 0/1875                       Loss D: 0.5416, loss G: 1.1437\n",
            "Epoch [46/50] Batch 0/1875                       Loss D: 0.7213, loss G: 0.7378\n",
            "Epoch [47/50] Batch 0/1875                       Loss D: 0.6838, loss G: 0.8234\n",
            "Epoch [48/50] Batch 0/1875                       Loss D: 0.7041, loss G: 0.8255\n",
            "Epoch [49/50] Batch 0/1875                       Loss D: 0.6079, loss G: 0.9816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlMp4EHQFwzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}